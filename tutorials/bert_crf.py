# -*- coding: utf-8 -*-
"""5-token_classification-è¯_ç¬¦å·_tokençº§åˆ«åˆ†ç±»ä»»åŠ¡.ipynb
åœ¨è¿è¡Œå•å…ƒæ ¼ä¹‹å‰ï¼Œå»ºè®®æ‚¨æŒ‰ç…§é¡¹ç›®readmeä¸­æç¤ºï¼Œå»ºç«‹ä¸€ä¸ªä¸“é—¨çš„pythonç¯å¢ƒç”¨äºå­¦ä¹ ï¼Œç„¶åå®‰è£…ä¾èµ–åº“ã€‚
"""
# ! pip install datasets transformers seqeval

"""å¦‚æœæ‚¨æ­£åœ¨æœ¬åœ°æ‰“å¼€è¿™ä¸ªnotebookï¼Œè¯·ç¡®ä¿æ‚¨è®¤çœŸé˜…è¯»å¹¶å®‰è£…äº†transformer-quick-start-zhçš„readmeæ–‡ä»¶ä¸­çš„æ‰€æœ‰ä¾èµ–åº“ã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/transformers/tree/master/examples/token-classification)æ‰¾åˆ°æœ¬notebookçš„å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒç‰ˆæœ¬ã€‚

# Fine-tuningå¾®è°ƒtransformeræ¨¡å‹ç”¨äºtokençº§çš„åˆ†ç±»ä»»åŠ¡ï¼ˆæ¯”å¦‚NERä»»åŠ¡ï¼‰

åœ¨è¿™ä¸ªnotebookä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨[ğŸ¤— Transformers](https://github.com/huggingface/transformers)ä¸­çš„æ¨¡å‹å»åštokençº§åˆ«çš„åˆ†ç±»é—®é¢˜ã€‚tokençº§åˆ«çš„åˆ†ç±»ä»»åŠ¡é€šå¸¸æŒ‡çš„æ˜¯ä¸ºä¸ºæ–‡æœ¬ä¸­çš„æ¯ä¸€ä¸ªtokené¢„æµ‹ä¸€ä¸ªæ ‡ç­¾ç»“æœã€‚ä¸‹å›¾å±•ç¤ºçš„æ˜¯ä¸€ä¸ªNERå®ä½“åè¯è¯†åˆ«ä»»åŠ¡ã€‚

![Widget inference representing the NER task](https://github.com/huggingface/notebooks/blob/master/examples/images/token_classification.png?raw=1)

æœ€å¸¸è§çš„tokençº§åˆ«åˆ†ç±»ä»»åŠ¡:

- NER (Named-entity recognition åè¯-å®ä½“è¯†åˆ«) åˆ†è¾¨å‡ºæ–‡æœ¬ä¸­çš„åè¯å’Œå®ä½“ (personäººå, organizationç»„ç»‡æœºæ„å, locationåœ°ç‚¹å...).
- POS (Part-of-speech taggingè¯æ€§æ ‡æ³¨) æ ¹æ®è¯­æ³•å¯¹tokenè¿›è¡Œè¯æ€§æ ‡æ³¨ (nounåè¯, verbåŠ¨è¯, adjectiveå½¢å®¹è¯...)
- Chunk (ChunkingçŸ­è¯­ç»„å—) å°†åŒä¸€ä¸ªçŸ­è¯­çš„tokensç»„å—æ”¾åœ¨ä¸€èµ·ã€‚

å¯¹äºä»¥ä¸Šä»»åŠ¡ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç®€å•çš„åŠ è½½æ•°æ®é›†ï¼ŒåŒæ—¶é’ˆå¯¹ç›¸åº”çš„ä»æ— ä½¿ç”¨transformerä¸­çš„`Trainer`æ¥å£å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

åªè¦é¢„è®­ç»ƒçš„transformeræ¨¡å‹æœ€é¡¶å±‚æœ‰ä¸€ä¸ªtokenåˆ†ç±»çš„ç¥ç»ç½‘ç»œå±‚ï¼ˆç”±äºtransformerçš„tokenizeræ–°ç‰¹æ€§ï¼Œè¿˜éœ€è¦å¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹æœ‰fast tokenizerï¼Œå‚è€ƒ[è¿™ä¸ªè¡¨](https://huggingface.co/transformers/index.html#bigtable)ï¼‰ï¼Œé‚£ä¹ˆæœ¬notebookç†è®ºä¸Šå¯ä»¥ä½¿ç”¨å„ç§å„æ ·çš„transformeræ¨¡å‹ï¼ˆ[æ¨¡å‹é¢æ¿](https://huggingface.co/models)ï¼‰ï¼Œè§£å†³ä»»ä½•tokençº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ã€‚

å¦‚æœæ‚¨æ‰€å¤„ç†çš„ä»»åŠ¡æœ‰æ‰€ä¸åŒï¼Œå¤§æ¦‚ç‡åªéœ€è¦å¾ˆå°çš„æ”¹åŠ¨ä¾¿å¯ä»¥ä½¿ç”¨æœ¬notebookè¿›è¡Œå¤„ç†ã€‚åŒæ—¶ï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‚¨çš„GPUæ˜¾å­˜æ¥è°ƒæ•´å¾®è°ƒè®­ç»ƒæ‰€éœ€è¦çš„btach sizeå¤§å°ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºã€‚
"""

task = "ner"  # éœ€è¦æ˜¯"ner", "pos" æˆ–è€… "chunk"
model_checkpoint = "distilbert-base-uncased"
batch_size = 16

"""## åŠ è½½æ•°æ®

æˆ‘ä»¬å°†ä¼šä½¿ç”¨[ğŸ¤— datasets](https://github.com/huggingface/datasets)åº“æ¥åŠ è½½æ•°æ®å’Œå¯¹åº”çš„è¯„æµ‹æ–¹å¼ã€‚æ•°æ®åŠ è½½å’Œè¯„æµ‹æ–¹å¼åŠ è½½åªéœ€è¦ç®€å•ä½¿ç”¨`load_dataset`å’Œ`load_metric`å³å¯ã€‚
"""

from datasets import load_dataset, load_metric

"""æœ¬notebookä¸­çš„ä¾‹å­ä½¿ç”¨çš„æ˜¯[CONLL 2003 dataset](https://www.aclweb.org/anthology/W03-0419.pdf)æ•°æ®é›†ã€‚è¿™ä¸ªnotebookåº”è¯¥å¯ä»¥å¤„ç†ğŸ¤— Datasetsåº“ä¸­çš„ä»»ä½•tokenåˆ†ç±»ä»»åŠ¡ã€‚å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯æ‚¨è‡ªå®šä¹‰çš„json/csvæ–‡ä»¶æ•°æ®é›†ï¼Œæ‚¨éœ€è¦æŸ¥çœ‹[æ•°æ®é›†æ–‡æ¡£](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files)æ¥å­¦ä¹ å¦‚ä½•åŠ è½½ã€‚è‡ªå®šä¹‰æ•°æ®é›†å¯èƒ½éœ€è¦åœ¨åŠ è½½å±æ€§åå­—ä¸Šåšä¸€äº›è°ƒæ•´ã€‚"""

datasets = load_dataset("conll2003")

"""è¿™ä¸ª`datasets`å¯¹è±¡æœ¬èº«æ˜¯ä¸€ç§[`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict)æ•°æ®ç»“æ„. å¯¹äºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œåªéœ€è¦ä½¿ç”¨å¯¹åº”çš„keyï¼ˆtrainï¼Œvalidationï¼Œtestï¼‰å³å¯å¾—åˆ°ç›¸åº”çš„æ•°æ®ã€‚"""

datasets

"""æ— è®ºæ˜¯åœ¨è®­ç»ƒé›†ã€éªŒè¯æœºè¿˜æ˜¯æµ‹è¯•é›†ä¸­ï¼Œdatasetséƒ½åŒ…å«äº†ä¸€ä¸ªåä¸ºtokensçš„åˆ—ï¼ˆä¸€èˆ¬æ¥è¯´æ˜¯å°†æ–‡æœ¬åˆ‡åˆ†æˆäº†å¾ˆå¤šè¯ï¼‰ï¼Œè¿˜åŒ…å«ä¸€ä¸ªåä¸ºlabelçš„åˆ—ï¼Œè¿™ä¸€åˆ—å¯¹åº”è¿™tokensçš„æ ‡æ³¨ã€‚

ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚
"""

datasets["train"][0]

"""æ‰€æœ‰çš„æ•°æ®æ ‡ç­¾labelséƒ½å·²ç»è¢«ç¼–ç æˆäº†æ•´æ•°ï¼Œå¯ä»¥ç›´æ¥è¢«é¢„è®­ç»ƒtransformeræ¨¡å‹ä½¿ç”¨ã€‚è¿™äº›æ•´æ•°çš„ç¼–ç æ‰€å¯¹åº”çš„å®é™…ç±»åˆ«å‚¨å­˜åœ¨`features`ä¸­ã€‚"""

datasets["train"].features[f"ner_tags"]

"""æ‰€ä»¥ä»¥NERä¸ºä¾‹ï¼Œ0å¯¹åº”çš„æ ‡ç­¾ç±»åˆ«æ˜¯â€Oâ€œï¼Œ 1å¯¹åº”çš„æ˜¯â€B-PERâ€œç­‰ç­‰ã€‚â€Oâ€œçš„æ„æ€æ˜¯æ²¡æœ‰ç‰¹åˆ«å®ä½“ï¼ˆno special entityï¼‰ã€‚æœ¬ä¾‹åŒ…å«4ç§å®ä½“ç±»åˆ«åˆ†åˆ«æ˜¯ï¼ˆPERã€ORGã€LOCï¼ŒMISCï¼‰ï¼Œæ¯ä¸€ç§å®ä½“ç±»åˆ«åˆåˆ†åˆ«æœ‰B-ï¼ˆå®ä½“å¼€å§‹çš„tokenï¼‰å‰ç¼€å’ŒI-ï¼ˆå®ä½“ä¸­é—´çš„tokenï¼‰å‰ç¼€ã€‚

- 'PER' for person
- 'ORG' for organization
- 'LOC' for location
- 'MISC' for miscellaneous

Since the labels are lists of `ClassLabel`, the actual names of the labels are nested in the `feature` attribute of the object above:
"""

label_list = datasets["train"].features[f"{task}_tags"].feature.names
label_list

"""ä¸ºäº†èƒ½å¤Ÿè¿›ä¸€æ­¥ç†è§£æ•°æ®é•¿ä»€ä¹ˆæ ·å­ï¼Œä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºã€‚"""

from datasets import ClassLabel, Sequence
import random
import pandas as pd
from IPython.display import display, HTML


def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset) - 1)
        while pick in picks:
            pick = random.randint(0, len(dataset) - 1)
        picks.append(pick)

    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):
            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])
    display(HTML(df.to_html()))


show_random_elements(datasets["train"])

"""## é¢„å¤„ç†æ•°æ®

åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚é¢„å¤„ç†çš„å·¥å…·å«`Tokenizer`ã€‚`Tokenizer`é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚

ä¸ºäº†è¾¾åˆ°æ•°æ®é¢„å¤„ç†çš„ç›®çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨`AutoTokenizer.from_pretrained`æ–¹æ³•å®ä¾‹åŒ–æˆ‘ä»¬çš„tokenizerï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ï¼š

- æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€ä¸€å¯¹åº”çš„tokenizerã€‚
- ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹checkpointå¯¹åº”çš„tokenizerçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¹Ÿä¸‹è½½äº†æ¨¡å‹éœ€è¦çš„è¯è¡¨åº“vocabularyï¼Œå‡†ç¡®æ¥è¯´æ˜¯tokens vocabularyã€‚

è¿™ä¸ªè¢«ä¸‹è½½çš„tokens vocabularyä¼šè¢«ç¼“å­˜èµ·æ¥ï¼Œä»è€Œå†æ¬¡ä½¿ç”¨çš„æ—¶å€™ä¸ä¼šé‡æ–°ä¸‹è½½ã€‚
"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

"""ä»¥ä¸‹ä»£ç è¦æ±‚tokenizerå¿…é¡»æ˜¯transformers.PreTrainedTokenizerFastç±»å‹ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨é¢„å¤„ç†çš„æ—¶å€™éœ€è¦ç”¨åˆ°fast tokenizerçš„ä¸€äº›ç‰¹æ®Šç‰¹æ€§ï¼ˆæ¯”å¦‚å¤šçº¿ç¨‹å¿«é€Ÿtokenizerï¼‰ã€‚

å‡ ä¹æ‰€æœ‰æ¨¡å‹å¯¹åº”çš„tokenizeréƒ½æœ‰å¯¹åº”çš„fast tokenizerã€‚æˆ‘ä»¬å¯ä»¥åœ¨[æ¨¡å‹tokenizerå¯¹åº”è¡¨](https://huggingface.co/transformers/index.html#bigtable)é‡ŒæŸ¥çœ‹æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯¹åº”çš„tokenizeræ‰€æ‹¥æœ‰çš„ç‰¹ç‚¹ã€‚
"""

import transformers

assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)

"""åœ¨[è¿™é‡Œbig table of models](https://huggingface.co/transformers/index.html#bigtable)æŸ¥çœ‹æ¨¡å‹æ˜¯å¦æœ‰fast tokenizerã€‚

tokenizeræ—¢å¯ä»¥å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œä¹Ÿå¯ä»¥å¯¹ä¸€å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œtokenizeré¢„å¤„ç†åå¾—åˆ°çš„æ•°æ®æ»¡è¶³é¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æ ¼å¼
"""

tokenizer("Hello, this is one sentence!")

tokenizer(["Hello", ",", "this", "is", "one", "sentence", "split", "into", "words", "."], is_split_into_words=True)

"""æ³¨æ„transformeré¢„è®­ç»ƒæ¨¡å‹åœ¨é¢„è®­ç»ƒçš„æ—¶å€™é€šå¸¸ä½¿ç”¨çš„æ˜¯subwordï¼Œå¦‚æœæˆ‘ä»¬çš„æ–‡æœ¬è¾“å…¥å·²ç»è¢«åˆ‡åˆ†æˆäº†wordï¼Œé‚£ä¹ˆè¿™äº›wordè¿˜ä¼šè¢«æˆ‘ä»¬çš„tokenizerç»§ç»­åˆ‡åˆ†ã€‚ä¸¾ä¸ªä¾‹å­ï¼š

"""

example = datasets["train"][4]
print(example["tokens"])

tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
print(tokens)

"""å•è¯"Zwingmann" å’Œ "sheepmeat"ç»§ç»­è¢«åˆ‡åˆ†æˆäº†3ä¸ªsubtokensã€‚

ç”±äºæ ‡æ³¨æ•°æ®é€šå¸¸æ˜¯åœ¨wordçº§åˆ«è¿›è¡Œæ ‡æ³¨çš„ï¼Œæ—¢ç„¶wordè¿˜ä¼šè¢«åˆ‡åˆ†æˆsubtokensï¼Œé‚£ä¹ˆæ„å‘³ç€æˆ‘ä»¬è¿˜éœ€è¦å¯¹æ ‡æ³¨æ•°æ®è¿›è¡Œsubtokensçš„å¯¹é½ã€‚åŒæ—¶ï¼Œç”±äºé¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æ ¼å¼çš„è¦æ±‚ï¼Œå¾€å¾€è¿˜éœ€è¦åŠ ä¸Šä¸€äº›ç‰¹æ®Šç¬¦å·æ¯”å¦‚ï¼š`[CLS]` å’Œ a `[SEP]`ã€‚
"""

len(example[f"{task}_tags"]), len(tokenized_input["input_ids"])

"""tokenizeræœ‰ä¸€ä¸ª` `word_ids`æ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ã€‚"""

print(tokenized_input.word_ids())

"""
æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œword_idså°†æ¯ä¸€ä¸ªsubtokensä½ç½®éƒ½å¯¹åº”äº†ä¸€ä¸ªwordçš„ä¸‹æ ‡ã€‚æ¯”å¦‚ç¬¬1ä¸ªä½ç½®å¯¹åº”ç¬¬0ä¸ªwordï¼Œç„¶åç¬¬2ã€3ä¸ªä½ç½®å¯¹åº”ç¬¬1ä¸ªwordã€‚ç‰¹æ®Šå­—ç¬¦å¯¹åº”äº†NOneã€‚æœ‰äº†è¿™ä¸ªlistï¼Œæˆ‘ä»¬å°±èƒ½å°†subtokenså’Œwordsè¿˜æœ‰æ ‡æ³¨çš„labelså¯¹é½å•¦ã€‚"""

word_ids = tokenized_input.word_ids()
aligned_labels = [-100 if i is None else example[f"{task}_tags"][i] for i in word_ids]
print(len(aligned_labels), len(tokenized_input["input_ids"]))

"""æˆ‘ä»¬é€šå¸¸å°†ç‰¹æ®Šå­—ç¬¦çš„labelè®¾ç½®ä¸º-100ï¼Œåœ¨æ¨¡å‹ä¸­-100é€šå¸¸ä¼šè¢«å¿½ç•¥æ‰ä¸è®¡ç®—lossã€‚

æˆ‘ä»¬æœ‰ä¸¤ç§å¯¹é½labelçš„æ–¹å¼ï¼š
- å¤šä¸ªsubtokenså¯¹é½ä¸€ä¸ªwordï¼Œå¯¹é½ä¸€ä¸ªlabel
- å¤šä¸ªsubtokensçš„ç¬¬ä¸€ä¸ªsubtokenå¯¹é½wordï¼Œå¯¹é½ä¸€ä¸ªlabelï¼Œå…¶ä»–subtokensç›´æ¥èµ‹äºˆ-100.

æˆ‘ä»¬æä¾›è¿™ä¸¤ç§æ–¹å¼ï¼Œé€šè¿‡`label_all_tokens = True`åˆ‡æ¢ã€‚

"""

label_all_tokens = True

"""
æœ€åæˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹åˆèµ·æ¥å˜æˆæˆ‘ä»¬çš„é¢„å¤„ç†å‡½æ•°ã€‚`is_split_into_words=True`åœ¨ä¸Šé¢å·²ç»ç»“æŸå•¦ã€‚"""


def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"{task}_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs


"""ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚"""

tokenize_and_align_labels(datasets['train'][:5])

"""æ¥ä¸‹æ¥å¯¹æ•°æ®é›†datasetsé‡Œé¢çš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå¤„ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨mapå‡½æ•°ï¼Œå°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚


"""

tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)

"""æ›´å¥½çš„æ˜¯ï¼Œè¿”å›çš„ç»“æœä¼šè‡ªåŠ¨è¢«ç¼“å­˜ï¼Œé¿å…ä¸‹æ¬¡å¤„ç†çš„æ—¶å€™é‡æ–°è®¡ç®—ï¼ˆä½†æ˜¯ä¹Ÿè¦æ³¨æ„ï¼Œå¦‚æœè¾“å…¥æœ‰æ”¹åŠ¨ï¼Œå¯èƒ½ä¼šè¢«ç¼“å­˜å½±å“ï¼ï¼‰ã€‚datasetsåº“å‡½æ•°ä¼šå¯¹è¾“å…¥çš„å‚æ•°è¿›è¡Œæ£€æµ‹ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å˜åŒ–ï¼Œå¦‚æœæ²¡æœ‰å˜åŒ–å°±ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå¦‚æœæœ‰å˜åŒ–å°±é‡æ–°å¤„ç†ã€‚ä½†å¦‚æœè¾“å…¥å‚æ•°ä¸å˜ï¼Œæƒ³æ”¹å˜è¾“å…¥çš„æ—¶å€™ï¼Œæœ€å¥½æ¸…ç†è°ƒè¿™ä¸ªç¼“å­˜ã€‚æ¸…ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨`load_from_cache_file=False`å‚æ•°ã€‚å¦å¤–ï¼Œä¸Šé¢ä½¿ç”¨åˆ°çš„`batched=True`è¿™ä¸ªå‚æ•°æ˜¯tokenizerçš„ç‰¹ç‚¹ï¼Œä»¥ä¸ºè¿™ä¼šä½¿ç”¨å¤šçº¿ç¨‹åŒæ—¶å¹¶è¡Œå¯¹è¾“å…¥è¿›è¡Œå¤„ç†ã€‚

## Fine-tuning the modelå¾®è°ƒæ¨¡å‹

æ—¢ç„¶æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½æˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚æ—¢ç„¶æˆ‘ä»¬æ˜¯åšseq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForTokenClassification` è¿™ä¸ªç±»ã€‚å’Œtokenizerç›¸ä¼¼ï¼Œ`from_pretrained`æ–¹æ³•åŒæ ·å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ¨¡å‹è¿›è¡Œç¼“å­˜ï¼Œå°±ä¸ä¼šé‡å¤ä¸‹è½½æ¨¡å‹å•¦ã€‚
"""

from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))

"""ç”±äºæˆ‘ä»¬å¾®è°ƒçš„ä»»åŠ¡æ˜¯tokenåˆ†ç±»ä»»åŠ¡ï¼Œè€Œæˆ‘ä»¬åŠ è½½çš„æ˜¯é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ‰€ä»¥ä¼šæç¤ºæˆ‘ä»¬åŠ è½½æ¨¡å‹çš„æ—¶å€™æ‰”æ‰äº†ä¸€äº›ä¸åŒ¹é…çš„ç¥ç»ç½‘ç»œå‚æ•°ï¼ˆæ¯”å¦‚ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç¥ç»ç½‘ç»œheadè¢«æ‰”æ‰äº†ï¼ŒåŒæ—¶éšæœºåˆå§‹åŒ–äº†tokenåˆ†ç±»çš„ç¥ç»ç½‘ç»œheadï¼‰ã€‚

ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª`Trainer`è®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦3ä¸ªè¦ç´ ï¼Œå…¶ä¸­æœ€é‡è¦çš„æ˜¯è®­ç»ƒçš„è®¾å®š/å‚æ•° [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§ã€‚
"""

args = TrainingArguments(
    f"test-{task}",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,
    weight_decay=0.01,
)

"""ä¸Šé¢evaluation_strategy = "epoch"å‚æ•°å‘Šè¯‰è®­ç»ƒä»£ç ï¼šæˆ‘ä»¬æ¯ä¸ªepcohä¼šåšä¸€æ¬¡éªŒè¯è¯„ä¼°ã€‚

ä¸Šé¢batch_sizeåœ¨è¿™ä¸ªnotebookä¹‹å‰å®šä¹‰å¥½äº†ã€‚

æœ€åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®æ”¶é›†å™¨data collatorï¼Œå°†æˆ‘ä»¬å¤„ç†å¥½çš„è¾“å…¥å–‚ç»™æ¨¡å‹ã€‚
"""

from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer)

"""
è®¾ç½®å¥½`Trainer`è¿˜å‰©æœ€åä¸€ä»¶äº‹æƒ…ï¼Œé‚£å°±æ˜¯æˆ‘ä»¬éœ€è¦å®šä¹‰å¥½è¯„ä¼°æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨[`seqeval`](https://github.com/chakki-works/seqeval) metricæ¥å®Œæˆè¯„ä¼°ã€‚å°†æ¨¡å‹é¢„æµ‹é€å…¥è¯„ä¼°ä¹‹å‰ï¼Œæˆ‘ä»¬ä¹Ÿä¼šåšä¸€äº›æ•°æ®åå¤„ç†ï¼š
"""

metric = load_metric("seqeval")

"""è¯„ä¼°çš„è¾“å…¥æ˜¯é¢„æµ‹å’Œlabelçš„list"""

labels = [label_list[i] for i in example[f"{task}_tags"]]
metric.compute(predictions=[labels], references=[labels])

"""å¯¹æ¨¡å‹é¢„æµ‹ç»“æœåšä¸€äº›åå¤„ç†ï¼š
- é€‰æ‹©é¢„æµ‹åˆ†ç±»æœ€å¤§æ¦‚ç‡çš„ä¸‹æ ‡
- å°†ä¸‹æ ‡è½¬åŒ–ä¸ºlabel
- å¿½ç•¥-100æ‰€åœ¨åœ°æ–¹

ä¸‹é¢çš„å‡½æ•°å°†ä¸Šé¢çš„æ­¥éª¤åˆå¹¶äº†èµ·æ¥ã€‚
"""

import numpy as np


def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }


"""æˆ‘ä»¬è®¡ç®—æ‰€æœ‰ç±»åˆ«æ€»çš„precision/recall/f1ï¼Œæ‰€ä»¥ä¼šæ‰”æ‰å•ä¸ªç±»åˆ«çš„precision/recall/f1 

å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥`Trainer`å³å¯

"""

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

"""è°ƒç”¨`train`æ–¹æ³•å¼€å§‹è®­ç»ƒ"""

trainer.train()

"""æˆ‘ä»¬å¯ä»¥å†æ¬¡ä½¿ç”¨`evaluate`æ–¹æ³•è¯„ä¼°ï¼Œå¯ä»¥è¯„ä¼°å…¶ä»–æ•°æ®é›†ã€‚"""

trainer.evaluate()

"""å¦‚æœæƒ³è¦å¾—åˆ°å•ä¸ªç±»åˆ«çš„precision/recall/f1ï¼Œæˆ‘ä»¬ç›´æ¥å°†ç»“æœè¾“å…¥ç›¸åŒçš„è¯„ä¼°å‡½æ•°å³å¯ï¼š"""

predictions, labels, _ = trainer.predict(tokenized_datasets["validation"])
predictions = np.argmax(predictions, axis=2)

# Remove ignored index (special tokens)
true_predictions = [
    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]
true_labels = [
    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]

results = metric.compute(predictions=true_predictions, references=true_labels)
# results

# æå®šï¼